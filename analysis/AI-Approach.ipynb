{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed7e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from curve_functions import FunctionProvider\n",
    "import sklearn.preprocessing as prep\n",
    "import typing\n",
    "\n",
    "RESULTS_BACKUP_PATH = '/home/mnapravnik/Documents/PhD/Efficiency-of-dragging-gestures/Results_backup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bcf6f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# first, gather all of the data related to drawn figures\n",
    "def get_drawn_figures(test:int=0):\n",
    "    \n",
    "    path = f'{RESULTS_BACKUP_PATH}{test}'\n",
    "    columns = [\n",
    "        'Username',\n",
    "        'TestIndex',\n",
    "        'Device',\n",
    "        'FigureID',\n",
    "        'Projection',\n",
    "        'ProjectionName',\n",
    "        'Order',\n",
    "        'Fullpath',\n",
    "        'Npoints'\n",
    "    ]\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for user_direntry in os.scandir(path):\n",
    "        # each directory is named after the user participant\n",
    "        _username = user_direntry.name\n",
    "#         _username = re.sub(' ', '_', _username)\n",
    "        \n",
    "        for device_direntry in os.scandir(user_direntry.path):\n",
    "            _device = device_direntry.name\n",
    "#             _device = re.sub(' ', '_', _device)\n",
    "                \n",
    "            for drawing_direntry in os.scandir(device_direntry.path):\n",
    "                _only_numbers_and_underscore = re.sub('[^0-9,_]', '', drawing_direntry.name)\n",
    "                _figureid, _projection, _order = _only_numbers_and_underscore.split('_')\n",
    "                _figureid = int(_figureid)\n",
    "                _projection = int(_projection)\n",
    "                _order = int(_order)\n",
    "                \n",
    "                # check how many drawn points there are\n",
    "                npoints = 0\n",
    "                with open(drawing_direntry.path) as file:\n",
    "                    npoints = len(file.readlines())\n",
    "                \n",
    "                # there seems to be a bug with polar projections\n",
    "                # where figures in projection 2 are named as projection 3\n",
    "                # and that happens only when\n",
    "                # figureid is 0, so just fix it here, on-the-go\n",
    "                if (_figureid == 0):\n",
    "                    if(_projection == 2):\n",
    "                        _projection = 3\n",
    "                    elif(_projection == 3):\n",
    "                        _projection = 2\n",
    "                _projection_name = 'Cartesian' if _projection in [0, 1] else 'Polar'\n",
    "                    \n",
    "                \n",
    "                # append the row to the end of the dataframe\n",
    "                df.loc[len(df)] = [\n",
    "                    _username,\n",
    "                    test,\n",
    "                    _device,\n",
    "                    _figureid,\n",
    "                    _projection,\n",
    "                    _projection_name,\n",
    "                    _order,\n",
    "                    drawing_direntry.path,\n",
    "                    npoints\n",
    "                ]\n",
    "#     df.set_index(['Username'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_timelogs():\n",
    "    df = pd.read_csv('timelogs.csv')\n",
    "#     display(df)\n",
    "    return df\n",
    "\n",
    "def merge_drawings_with_len_and_kappa(df):\n",
    "    len_df = pd.read_json('index_of_difficulty-length.json')\n",
    "    kappa_df = pd.read_json('index_of_difficulty-kappa.json')\n",
    "    # in these two jsons, the column names are projections\n",
    "    # i.e. [0, 1, 2, 3]\n",
    "    # and indices are figure IDs\n",
    "    \n",
    "    df['Length'] = -1.0\n",
    "    df['Kappa'] = -1.0\n",
    "    \n",
    "    for figureid in len_df.index:\n",
    "        for projection_id in len_df.columns:\n",
    "            df_tmp_ids = df.query(\n",
    "                f'FigureID == {figureid} and ' +\n",
    "                f'Projection == {projection_id}'\n",
    "            ).index\n",
    "            \n",
    "            df.loc[df_tmp_ids, 'Kappa'] = kappa_df.loc[figureid, projection_id]\n",
    "            df.loc[df_tmp_ids, 'Length'] = len_df.loc[figureid, projection_id]     \n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_timelogs_with_drawings(timelogs_df, drawings_df):\n",
    "    drawings_df['Drawing time'] = 0.0\n",
    "    drawings_df['Timestamp'] = 0\n",
    "\n",
    "    # sort drawings by the order they appeared in\n",
    "    drawings_df.sort_values('Order', inplace=True)\n",
    "    timelogs_df.sort_values('Logging timestamp', inplace=True)\n",
    "    \n",
    "    for _idx in timelogs_df.index:\n",
    "        timelog_entry = timelogs_df.loc[_idx]\n",
    "        username = timelog_entry['Participant name']\n",
    "        device = timelog_entry['Device']\n",
    "        figureid = timelog_entry['Function ID']\n",
    "        projection_name = timelog_entry['Function projection']\n",
    "        test_index = timelog_entry['Test mode']\n",
    "        # there were two drawings for each function\n",
    "        # so 'drawing' will have two entries\n",
    "        # sorted in chronological order\n",
    "        drawing = drawings_df.query(\n",
    "            f'Device == \"{device}\" and '\n",
    "            f'FigureID == {figureid} and '\n",
    "            f'Username == \"{username}\" and '\n",
    "            f'ProjectionName == \"{projection_name}\" and '\n",
    "            f'TestIndex == {test_index}'\n",
    "        )\n",
    "        \n",
    "        _drwg_id = drawing.index[0]\n",
    "        \n",
    "        # if this row has already been filled\n",
    "        # i.e. if this drawing was already paired with a timestamp\n",
    "        # then the other (the second) drawing has to be paired now\n",
    "        if drawing.loc[_drwg_id, 'Timestamp'] > 0:\n",
    "            _drwg_id = drawing.index[1]\n",
    "        \n",
    "        drawings_df.loc[_drwg_id, 'Timestamp'] = timelog_entry['Logging timestamp']\n",
    "        drawings_df.loc[_drwg_id, 'Drawing time'] = timelog_entry['Drawing time']\n",
    "    drawings_df.sort_values(['Timestamp', 'Username'], inplace=True)\n",
    "    return drawings_df\n",
    "    \n",
    "df = get_drawn_figures(0)\n",
    "df = pd.concat([df, get_drawn_figures(1)], ignore_index=True)\n",
    "\n",
    "timelogs = get_timelogs()\n",
    "df = merge_timelogs_with_drawings(timelogs, df)\n",
    "\n",
    "df = merge_drawings_with_len_and_kappa(df)\n",
    "\n",
    "display(df)\n",
    "display(df.Username.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1f9731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_drawn_coordinates(df_entry, scale:bool=False):\n",
    "    _drawingx, _drawingy = [], []\n",
    "    with open(df_entry.Fullpath) as file:\n",
    "        for line in file:\n",
    "            pointx, pointy = line.split()\n",
    "            pointx = float(pointx)\n",
    "            pointy = float(pointy)\n",
    "            _drawingx.append(pointx)\n",
    "            _drawingy.append(pointy)\n",
    "    return _drawingx, _drawingy\n",
    "\n",
    "def scale_coordinates(df_entry, x, y):\n",
    "    from display_properties import CARTESIAN_PLOT_LIMITS, POLAR_PLOT_LIMITS\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    if df_entry.Projection in [0,1]:\n",
    "        # Cartesian plot\n",
    "        y = (y - CARTESIAN_PLOT_LIMITS['y'][0]) / CARTESIAN_PLOT_LIMITS['y'][1]\n",
    "        x = (x - CARTESIAN_PLOT_LIMITS['x'][0]) / CARTESIAN_PLOT_LIMITS['x'][1]\n",
    "    else:\n",
    "        # Polar plot\n",
    "        # due to scaling, polar plots will look a bit idiotic\n",
    "        y = (y - POLAR_PLOT_LIMITS['y'][0]) / POLAR_PLOT_LIMITS['y'][1]\n",
    "        x = (x - POLAR_PLOT_LIMITS['x'][0]) / POLAR_PLOT_LIMITS['x'][1]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def unscale_coordinates(x, y, projection):\n",
    "    from display_properties import CARTESIAN_PLOT_LIMITS, POLAR_PLOT_LIMITS\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    if projection in [0,1]:\n",
    "        # Cartesian plot\n",
    "        y = y * CARTESIAN_PLOT_LIMITS['y'][1] + CARTESIAN_PLOT_LIMITS['y'][0]\n",
    "        x = y * CARTESIAN_PLOT_LIMITS['x'][1] + CARTESIAN_PLOT_LIMITS['x'][0]\n",
    "    else:\n",
    "        # Polar plot\n",
    "        y = y * POLAR_PLOT_LIMITS['y'][1] + POLAR_PLOT_LIMITS['y'][0]\n",
    "        x = y * POLAR_PLOT_LIMITS['x'][1] + POLAR_PLOT_LIMITS['x'][0]\n",
    "    return x, y \n",
    "    \n",
    "\n",
    "def get_real_coordinates(df_entry, x):\n",
    "    fp = FunctionProvider()\n",
    "    difficulty = int(df_entry.FigureID / 2)\n",
    "    task = int(df_entry.FigureID % 2)\n",
    "    func_y = fp.provide_function_y(difficulty, task, x, df_entry.Projection)\n",
    "    return x, func_y\n",
    "\n",
    "\n",
    "# Inspect drawn vs. original data\n",
    "def plot_function_and_drawing(df_entry):\n",
    "    _drawingx, _drawingy = get_drawn_coordinates(df_entry)\n",
    "    \n",
    "    _, func_y = get_real_coordinates(df_entry, _drawingx)\n",
    "#     _drawingx, _drawingy = scale_coordinates(df_entry, _drawingx, _drawingy)\n",
    "#     _, func_y = scale_coordinates(df_entry, _drawingx, func_y)\n",
    "    \n",
    "    if df_entry.ProjectionName == 'Cartesian':\n",
    "        # cartesian projection\n",
    "        fig, ax = plt.subplots()\n",
    "    else:\n",
    "        # polar projection\n",
    "        fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "    ax = fig.gca()\n",
    "    ax.plot(_drawingx, func_y, label='Original')\n",
    "    ax.plot(_drawingx, _drawingy, label='Drawn', color='gray')\n",
    "    ax.set_title(\n",
    "        f'FigureID: {df_entry.FigureID}, Projection: {df_entry.Projection}'\n",
    "    )\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "entry = df.query('Projection == 3 and FigureID == 2').iloc[8]\n",
    "print('Entry ID:', entry.name)\n",
    "plot_function_and_drawing(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeecae8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare data for training\n",
    "columncnt = 4096\n",
    "from display_properties import X_RANGE\n",
    "# this is the X RANGE for ALLLLL (!!!) OF THE functions\n",
    "# both the drawn and the generated (original) functions\n",
    "X_FOR_ALL_FUNC = np.linspace(X_RANGE['start'], X_RANGE['end'], columncnt)\n",
    "\n",
    "def prepare_points(df, columncnt:int):\n",
    "    \"\"\"\n",
    "    This accepts the original dataframe as input\n",
    "    and returns the points encoded and ready for training.\n",
    "    All coordinates are scaled between [0, 1]\n",
    "    and scaled to be the same length (columncnt is the length)\n",
    "    \n",
    "    Returns three values:\n",
    "    - the value on the X axis\n",
    "    - the value drawn by the user\n",
    "    - the real value of the function\n",
    "    \n",
    "    \"\"\"\n",
    "    def convert_to_same_size(columncnt:int, x, y):\n",
    "        resultx = np.zeros(columncnt)\n",
    "        resulty = np.zeros(columncnt)\n",
    "        if(len(x) < columncnt):\n",
    "            # this has the effect of zero-padding\n",
    "            # because were adding the drawings to an array of zeros\n",
    "            resultx[:len(x)] = x\n",
    "            resulty[:len(y)] = y\n",
    "        else:\n",
    "            # otherwise crop the drawing\n",
    "            resultx = x[:columncnt]\n",
    "            resulty = y[:columncnt]\n",
    "        return resultx, resulty\n",
    "    \n",
    "    x = []\n",
    "    drawn_y = []\n",
    "    real_y = []\n",
    "    for index in df.index:\n",
    "        entry = df.loc[index]\n",
    "        # first, get the drawn coordinates\n",
    "        _drawingx, _drawingy = get_drawn_coordinates(entry)\n",
    "        tmp1, tmp2 = _drawingx.copy(), _drawingy.copy()\n",
    "        \n",
    "        # now, get the real function coordinates\n",
    "        func_x = X_FOR_ALL_FUNC\n",
    "        _, func_y = get_real_coordinates(entry, func_x)\n",
    "        \n",
    "        # interpolate the drawn curves so they always have the same x values\n",
    "        if entry.ProjectionName == 'Cartesian':\n",
    "            _drawingy = np.interp(func_x, _drawingx, _drawingy)\n",
    "        else:\n",
    "            # polar function have a period of 2pi so interpolate them as such\n",
    "            _drawingy = np.interp(func_x, _drawingx, _drawingy, period=360)\n",
    "        _drawingx = func_x\n",
    "        \n",
    "        # test to see if the interpolated function still looks like the original\n",
    "#         if entry.ProjectionName == 'Cartesian':\n",
    "#             # cartesian projection\n",
    "#             fig, ax = plt.subplots()\n",
    "#         else:\n",
    "#             # polar projection\n",
    "#             fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "#         ax.plot(func_x, func_y, label='original func')\n",
    "#         ax.plot(tmp1, tmp2, label='original drawn')\n",
    "#         ax.plot(_drawingx, _drawingy, label='interp')\n",
    "#         ax.legend()\n",
    "#         plt.show()\n",
    "                \n",
    "        # scale coords between 0 and 1\n",
    "        _, func_y = scale_coordinates(entry, _drawingx, func_y)\n",
    "        _drawingx, _drawingy = scale_coordinates(entry, _drawingx, _drawingy)\n",
    "#         # add zero padding where necessary\n",
    "#         _, func_y = convert_to_same_size(columncnt, x=_drawingx, y=func_y)\n",
    "#         _drawingx, _drawingy = convert_to_same_size(columncnt, x=_drawingx, y=_drawingy)\n",
    "        \n",
    "        x.append(_drawingx)\n",
    "        drawn_y.append(_drawingy)\n",
    "        real_y.append(func_y)\n",
    "        \n",
    "    x = np.array(x)\n",
    "    drawn_y = np.array(drawn_y)\n",
    "    real_y = np.array(real_y)\n",
    "    \n",
    "    return x, drawn_y, real_y\n",
    "\n",
    "def prepare_dataset_for_user_classification(df, columncnt:int):\n",
    "    x, drawn_y, real_y = prepare_points(df, columncnt)\n",
    "    y = np.abs(drawn_y - real_y)\n",
    "    \n",
    "    # now prepare the rest: i.e. the device used, the projection and the usernames\n",
    "    proj_encoder = prep.LabelBinarizer()\n",
    "    proj_encoded = proj_encoder.fit_transform(df.ProjectionName)\n",
    "    user_encoder = prep.LabelBinarizer()\n",
    "    user_encoded = user_encoder.fit_transform(df.Username)\n",
    "    # the devices only have two values: Mouse & Graphic tablet\n",
    "    # so use Label Encoder instead\n",
    "    device_encoder = prep.LabelBinarizer()\n",
    "    device_encoded = device_encoder.fit_transform(df.Device)\n",
    "    \n",
    "    device_encoded = np.array(device_encoded, dtype='int16')\n",
    "    user_encoded = np.array(user_encoded, dtype='int16')\n",
    "    proj_encoded = np.array(proj_encoded, dtype='int16')\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    Entry = namedtuple('Entry', ['encoded', 'encoder', 'name'])\n",
    "    entries_to_add_to_df: typing.List[Entry] = [\n",
    "        Entry(proj_encoded, proj_encoder, 'Projection'),\n",
    "        Entry(user_encoded, user_encoder, 'Username'),\n",
    "        Entry(device_encoded, device_encoder, 'Device'),\n",
    "#         Entry(x, None, 'Xcoord'),\n",
    "        Entry(drawn_y, None, 'drawnY'),\n",
    "        Entry(real_y, None, 'realY'),\n",
    "#         Entry(y, None, 'YDiff')\n",
    "    ]\n",
    "    \n",
    "    # now, join everything into a single dataframe\n",
    "    # first, get column names to use\n",
    "    columns = []\n",
    "    for encoded, encoder, name in entries_to_add_to_df:\n",
    "        col_names = []\n",
    "        for _idx in range(np.shape(encoded)[1]):\n",
    "            _tmp = _idx\n",
    "            if(encoder is not None):\n",
    "                _tmp = encoder.classes_[_idx]\n",
    "            col_names.append(f'{name}_{_tmp}')\n",
    "        \n",
    "        columns = columns + col_names\n",
    "    # horizontally stack all of the encoded values\n",
    "    data = np.hstack([entry.encoded for entry in entries_to_add_to_df])\n",
    "    df = pd.DataFrame(\n",
    "        columns=columns,\n",
    "        data=data,\n",
    "        index=df.index\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_to_use = df.copy()\n",
    "# df_to_use = df.copy()\n",
    "display(df_to_use)\n",
    "encoded_df = prepare_dataset_for_user_classification(\n",
    "    df_to_use,\n",
    "    columncnt=5000\n",
    ")\n",
    "display(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd5449",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = encoded_df[df_to_use.TestIndex == 0]\n",
    "test_df = encoded_df[df_to_use.TestIndex == 1]\n",
    "\n",
    "train_dfY = train_df.filter(regex='Username_*')\n",
    "train_dfX = train_df.drop(train_dfY.columns, axis=1)\n",
    "\n",
    "test_dfY = test_df.filter(regex='Username_*')\n",
    "test_dfX = test_df.drop(test_dfY.columns, axis=1)\n",
    "\n",
    "display(train_dfX)\n",
    "display(test_dfX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a684ceaf",
   "metadata": {},
   "source": [
    "# Classification model in PyTorch\n",
    "https://stackabuse.com/introduction-to-pytorch-for-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432aa841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# limit CPU usage\n",
    "torch.set_num_threads(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df28d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes:typing.List[int], input_size:int, output_size=int, dropout=0.3):\n",
    "        super().__init__()\n",
    "        _all_layers = []\n",
    "        for _size in hidden_layer_sizes:\n",
    "            _all_layers.append(nn.Linear(input_size, _size))\n",
    "            _all_layers.append(nn.ReLU(inplace=True))\n",
    "            _all_layers.append(nn.BatchNorm1d(_size))\n",
    "            # apply dropout if specified so\n",
    "            if dropout > 0:\n",
    "                _all_layers.append(nn.Dropout(dropout))\n",
    "            # update the input size of the following layer\n",
    "            input_size = _size\n",
    "        # this is the output layer\n",
    "        _all_layers.append(nn.Linear(in_features=hidden_layer_sizes[-1], out_features=output_size))\n",
    "        # apply another activation function to the output layer, so that the output is scaled\n",
    "#         _all_layers.append(nn.Softmax(dim=1))\n",
    "        \n",
    "        # finally, create a model that has all these layers\n",
    "        # aligned in a sequence\n",
    "        # this means the layers will be called/processed one after the other\n",
    "        self.layers = nn.Sequential(*_all_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         print(x[0:10])\n",
    "        return self.layers(x)\n",
    "\n",
    "# convert the pandas df into torch tensor\n",
    "torch_train_dfX = torch.tensor(np.array(train_dfX, dtype='float32'))\n",
    "torch_train_dfY = torch.tensor(np.array(train_dfY, dtype='float32'))\n",
    "torch_test_dfX = torch.tensor(np.array(test_dfX, dtype='float32'))\n",
    "torch_test_dfY = torch.tensor(np.array(test_dfY, dtype='float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe614b77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_size = torch_train_dfX.shape[1]\n",
    "output_size = torch_train_dfY.shape[1]\n",
    "display(input_size, output_size)\n",
    "model = ClassificationModel([2**10, 2**8, 2**6], input_size=input_size, output_size=output_size, dropout=0.5)\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b0806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "epochs = 15\n",
    "aggregated_losses = []\n",
    "\n",
    "for _epoch_idx in range(epochs):\n",
    "    y_pred = model(torch_train_dfX)\n",
    "    loss_in_this_epoch = loss_function(y_pred, torch_train_dfY)\n",
    "    aggregated_losses.append(loss_in_this_epoch)\n",
    "    \n",
    "    _end = '\\r'\n",
    "    if (_epoch_idx + 1) % 10 == 0:\n",
    "        _end = '\\n'\n",
    "    print(f'Epoch {_epoch_idx+1:4} ===> loss {loss_in_this_epoch.item():10.8f}', end=_end)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # propagate the calculated loss backward through the network\n",
    "    loss_in_this_epoch.backward()\n",
    "    # update the weights in the network\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d02c66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(epochs), [loss.detach().numpy() for loss in aggregated_losses])\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    model.eval()\n",
    "    y_val = model(torch_test_dfX)\n",
    "    loss = loss_function(y_val, torch_test_dfY)\n",
    "    \n",
    "    _cnt = 10\n",
    "    argmax_y_val = np.argmax(y_val, axis=1)\n",
    "    argmax_y_true = np.argmax(torch_test_dfY, axis=1)\n",
    "    print(f'Loss on test data: {loss}')\n",
    "    print(argmax_y_val[:_cnt])\n",
    "    print(argmax_y_true[:_cnt])\n",
    "    print(y_val[:_cnt])\n",
    "    display(test_dfY)\n",
    "    \n",
    "    print(classification_report(argmax_y_true, argmax_y_val))\n",
    "    print(confusion_matrix(argmax_y_true, argmax_y_val))\n",
    "    \n",
    "    for i in range(100):\n",
    "        real_user_index = argmax_y_true[i]\n",
    "        predicted_user_index = argmax_y_val[i]\n",
    "        if(real_user_index != predicted_user_index):\n",
    "            # plot how the predicted and the real user\n",
    "            # drew this figure\n",
    "            _df_entry = df_to_use.query('TestIndex == 1').iloc[i]\n",
    "            x_real, y_real = get_drawn_coordinates(_df_entry)\n",
    "            real_user_name = _df_entry.Username\n",
    "            \n",
    "            predicted_user_name = list(test_dfY.columns)[predicted_user_index]\n",
    "            predicted_user_name = re.sub('Username_', '', predicted_user_name)\n",
    "            _predicted_user_df_entry = df.query(\n",
    "                f'Username == \"{predicted_user_name}\" and Device == \"{_df_entry.Device}\"' +\n",
    "                f' and FigureID == {_df_entry.FigureID} and Projection == {_df_entry.Projection}'\n",
    "            ).sample(1).iloc[0]\n",
    "            x_pred, y_pred = get_drawn_coordinates(_predicted_user_df_entry)\n",
    "            \n",
    "            if _df_entry.ProjectionName == 'Cartesian':\n",
    "                # cartesian projection\n",
    "                fig, ax = plt.subplots()\n",
    "            else:\n",
    "                # polar projection\n",
    "                fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "            ax = fig.gca()\n",
    "            ax.plot(x_real, y_real, label=f'Real user {real_user_name}')\n",
    "            ax.plot(x_pred, y_pred, label=f'Pred user {predicted_user_name}')\n",
    "            ax.legend()\n",
    "            plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d977ec2f",
   "metadata": {},
   "source": [
    "# Autoencoder to generate new drawings\n",
    "https://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c7b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEModel(nn.Module):\n",
    "    def __init__(self, hidden_layer_sizes:typing.List[int], in_size:tuple, out_size:tuple, dropout:float=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        def get_layers(_layer_sizes:typing.List[int], input_size:int, reverse:bool=False):\n",
    "            _layers = []\n",
    "            for i in _layer_sizes:\n",
    "#                 _layers.append(nn.Linear(in_features=input_size, out_features=i))\n",
    "#                 _layers.append(nn.LeakyReLU(inplace=True))\n",
    "#                 _layers.append(nn.BatchNorm1d(i))\n",
    "                _layers.append(nn.Conv1d(input_size, i, kernel_size=3, stride=1, padding='same'))\n",
    "                _layers.append(nn.LeakyReLU(inplace=True))\n",
    "                if reverse is False:\n",
    "                    _layers.append(nn.MaxPool1d(kernel_size=2))\n",
    "                else:\n",
    "#                     _layers.append(nn.Upsample(scale_factor=2))\n",
    "                    _layers.append(nn.Upsample(scale_factor=2))\n",
    "#                 _layers.append(nn.ReLU(inplace=True))\n",
    "                if dropout > 0:\n",
    "                    _layers.append(nn.Dropout(dropout))\n",
    "                # update the input size of the following layer\n",
    "                input_size = i\n",
    "            return _layers\n",
    "            \n",
    "        # encoder part of the network\n",
    "        _encoder_layers = get_layers(hidden_layer_sizes, input_size=in_size[0])\n",
    "        \n",
    "        self.bottleneck_input = in_size[1] // (2 ** len(hidden_layer_sizes)) * hidden_layer_sizes[-1]\n",
    "        self.bottleneck_size = 8\n",
    "        \n",
    "        # the bottleneck which will serve for getting the latent representation\n",
    "        self.fc1 = nn.Linear(self.bottleneck_input, self.bottleneck_size)\n",
    "        # THIS is the bottleneck layer\n",
    "        self.fc_bottle = nn.Linear(self.bottleneck_size, self.bottleneck_size)\n",
    "        # layer for passing data back to conv1d\n",
    "        self.fc2 = nn.Linear(self.bottleneck_size, self.bottleneck_input)\n",
    "        \n",
    "        # decoder part of network\n",
    "        _reversed_lyrs = list(reversed(hidden_layer_sizes))\n",
    "#         _reversed_lyrs.append(out_size[0])\n",
    "        _decoder_layers = get_layers(_reversed_lyrs, input_size=hidden_layer_sizes[-1], reverse=True)\n",
    "#         _decoder_layers.append(nn.Linear(in_features=_reversed_lyrs[-1], out_features=out_size))\n",
    "        _decoder_layers.append(\n",
    "            nn.Conv1d(_reversed_lyrs[-1], out_size[0], kernel_size=3, stride=1, padding='same')\n",
    "        )\n",
    "        \n",
    "        self.encoder = nn.Sequential(*_encoder_layers)\n",
    "        self.decoder = nn.Sequential(*_decoder_layers)\n",
    "        self.encoder_layers = _encoder_layers\n",
    "        self.decoder_layers = _decoder_layers\n",
    "     \n",
    "    \n",
    "    def get_latent(self, x):\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        batch, n_channels, n_features = x.shape\n",
    "        # flatten x so that it's shape is (batch, n_channels*n_features)\n",
    "        # and as such can be passed to the fc layer\n",
    "        x = x.view(batch, n_channels * n_features)\n",
    "        hidden = self.fc1(x)\n",
    "        \n",
    "        z = self.fc_bottle(hidden) # now we have the latent representation\n",
    "        \n",
    "        # return the latent rep and the original shape\n",
    "        # we need the original shape to reshape the data\n",
    "        # for decoder input\n",
    "        return z, (batch, n_channels, n_features)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        z, (batch, n_channels, n_features) = self.get_latent(x)\n",
    "\n",
    "        z = self.fc2(z) \n",
    "        # as the data was flattened, we need to return it to it's previous shape\n",
    "        # so it can be passed onto the decoder\n",
    "        z = z.view(-1, n_channels, n_features)\n",
    "        x = z\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        reconstruction = torch.sigmoid(x)\n",
    "        return reconstruction, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da776129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_for_ae(df, columncnt):\n",
    "    x, drawn_y, real_y = prepare_points(df, columncnt=columncnt)\n",
    "    x = [[l] for l in x]\n",
    "    drawn_y = [[l] for l in drawn_y]\n",
    "    real_y = [[l] for l in real_y]\n",
    "    \n",
    "    x = np.array(x, dtype='float32')\n",
    "    drawn_y = np.array(drawn_y, dtype='float32')\n",
    "    real_y = np.array(real_y, dtype='float32')\n",
    "    \n",
    "    # input should contain the value on the x axis and the\n",
    "    # original (real) value\n",
    "#     X = np.hstack([x, real_y])\n",
    "    X = drawn_y\n",
    "    \n",
    "    # the output should contain the drawn value\n",
    "#     Y = np.hstack([x, drawn_y])\n",
    "    Y = drawn_y\n",
    "    print(np.shape(X), np.shape(Y))\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def perform_train(trainx, trainy, testx, testy):    \n",
    "    model = AEModel([2, 4, 8, 16, 32], in_size=trainx.shape[1:], out_size=trainy.shape[1:],  dropout=0)\n",
    "    display(model)\n",
    "    \n",
    "    lr = 0.001\n",
    "    batch_size = 32\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    bce = nn.BCELoss(reduction='sum')\n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    # dataloaders with batch sizes\n",
    "    trainx_loader = torch.utils.data.DataLoader(torch.tensor(trainx), batch_size=batch_size, shuffle=False)\n",
    "    trainy_loader = torch.utils.data.DataLoader(torch.tensor(trainy), batch_size=batch_size, shuffle=False)\n",
    "    testx_loader = torch.utils.data.DataLoader(torch.tensor(testx), batch_size=batch_size, shuffle=False)\n",
    "    testy_loader = torch.utils.data.DataLoader(torch.tensor(testy), batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    def loss_function(bce_loss, mu, log_var):\n",
    "        BCE = bce_loss\n",
    "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        # the final loss is binary cross entropy + kullback leibler divergence\n",
    "        return BCE + KLD\n",
    "    \n",
    "    def fit(model, xdataloader, ydataloader, validation:bool=False):\n",
    "        i = 0\n",
    "        total_epoch_loss = 0.0\n",
    "        for x, y in zip(xdataloader, ydataloader):\n",
    "            if validation is False:\n",
    "                optimizer.zero_grad()\n",
    "            reconstruction, mu, log_var = model(x)\n",
    "            bce_loss = bce(reconstruction, y)\n",
    "            loss = mse(reconstruction, y)\n",
    "#             loss = loss_function(bce_loss, mu, log_var)\n",
    "            \n",
    "            total_epoch_loss += loss\n",
    "\n",
    "            if i == 0:\n",
    "                fig = plt.figure()\n",
    "                ax = fig.gca()\n",
    "                import random\n",
    "                _ind = random.randint(0, len(x)-1)\n",
    "                tmp = x[_ind][0]\n",
    "                tmp2 = y[_ind][0]\n",
    "                tmp3 = reconstruction[_ind][0]\n",
    "                tmp3 = [t.detach().numpy() for t in tmp3]\n",
    "                ax.plot(X_FOR_ALL_FUNC, tmp)\n",
    "                ax.plot(X_FOR_ALL_FUNC, tmp2)\n",
    "                ax.plot(X_FOR_ALL_FUNC, tmp3)\n",
    "                ax.set_title(f'index {_ind}')\n",
    "                plt.show()\n",
    "                i += 1\n",
    "\n",
    "            if validation is False:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        total_epoch_loss /= len(xdataloader.dataset)\n",
    "        return total_epoch_loss\n",
    "\n",
    "    epochs = 50\n",
    "    train_losses, val_losses = [], []\n",
    "    for _epoch_idx in range(epochs):\n",
    "        # switch model to training mode\n",
    "        model.train()\n",
    "        train_epoch_loss = fit(model, trainx_loader, trainy_loader, False)\n",
    "        train_losses.append(train_epoch_loss)\n",
    "        \n",
    "        # and now val mode\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_epoch_loss = fit(model, testx_loader, testy_loader, True)\n",
    "            val_losses.append(val_epoch_loss)\n",
    "        \n",
    "        print(f'Epoch: {_epoch_idx+1:5} / {epochs};',\n",
    "              f' train loss: {train_epoch_loss:.5f}, val loss: {val_epoch_loss:.5f}')\n",
    "    \n",
    "    def plot_training_stats():\n",
    "        fig = plt.figure()\n",
    "        ax = fig.gca()\n",
    "        ax.plot(range(epochs), [loss.detach().numpy() for loss in train_losses], label='Train')\n",
    "        ax.plot(range(epochs), [loss.detach().numpy() for loss in val_losses], label='Val')\n",
    "        ax.set_title('Losses')\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    plot_training_stats()\n",
    "    return model\n",
    "\n",
    "def do_everything(columncnt):\n",
    "    trainx, trainy = prepare_dataset_for_ae(df, columncnt)\n",
    "    testx, testy   = prepare_dataset_for_ae(df, columncnt)\n",
    "    model = perform_train(trainx, trainy, testx, testy)\n",
    "\n",
    "    return model, (trainx, trainy, testx, testy)\n",
    "\n",
    "columncnt = len(X_FOR_ALL_FUNC)\n",
    "ae_model, datasets = do_everything(columncnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528cbdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def draw_prediction(testx, testy, model, columncnt):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    import random\n",
    "    _ind = random.randint(0, len(testx))\n",
    "    real_func_x = X_FOR_ALL_FUNC\n",
    "    real_func_y = testx[_ind][0]\n",
    "    ax.plot(real_func_x, real_func_y, label='X')\n",
    "    \n",
    "    drawn_func_x = X_FOR_ALL_FUNC\n",
    "    drawn_func_y = testy[_ind][0]\n",
    "    ax.plot(drawn_func_x, drawn_func_y, label='Y')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted, _, _ = model(torch.tensor(testx))\n",
    "        \n",
    "#         pred_func_x = predicted[_ind, :columncnt]\n",
    "        pred_func_x = X_FOR_ALL_FUNC\n",
    "        pred_func_y = predicted[_ind][0]\n",
    "        ax.plot(pred_func_x, pred_func_y, label='Predicted')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "draw_prediction(datasets[2], datasets[3], ae_model, columncnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d45a87b",
   "metadata": {},
   "source": [
    "# Predicting the time needed to draw a curve using CNN\n",
    "TF/Keras: https://towardsdatascience.com/house-prices-prediction-using-deep-learning-dea265cc3154\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a18f43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TimePredictionModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layer_sizes: typing.List[int],\n",
    "        in_size:int,\n",
    "        dropout:float=0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        _all_layers = []\n",
    "        for _lyr_size in hidden_layer_sizes:\n",
    "            _all_layers.append(nn.Linear(in_features=in_size, out_features=_lyr_size))\n",
    "            _all_layers.append(nn.LeakyReLU(inplace=True))\n",
    "            \n",
    "            if(dropout > 0):\n",
    "                _all_layers.append(nn.Dropout(p=dropout))\n",
    "            in_size = _lyr_size\n",
    "        \n",
    "        # at the end, there is a layer with only one neuron: which is the output\n",
    "        # and the output is the predicted time\n",
    "        _all_layers.append(nn.Linear(in_features=in_size, out_features=1))\n",
    "        self.layers = nn.Sequential(*_all_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "def prepare_dataset_for_time_pred(df: pd.DataFrame, include_fig_embeddings:bool=False):\n",
    "    \"\"\"Returns X any Y (as tuple, in respective order)\"\"\"\n",
    "    \n",
    "    # first of all, find unique figures (through FigureID and Projection)\n",
    "    figureids = df['FigureID'].unique()\n",
    "    projectionids = df['Projection'].unique()\n",
    "    \n",
    "    data_for_nn = pd.DataFrame(columns=['FigureID', 'Projection', 'Length', 'Kappa', 'AvgDrawTime'])\n",
    "    embeddings_arr = []\n",
    "    \n",
    "    for figureid in figureids:\n",
    "        for projectionid in projectionids:\n",
    "            all_drawings_for_this_func = df.query(\n",
    "                f'FigureID == {figureid} and ' +\n",
    "                f'Projection == {projectionid}'\n",
    "            )\n",
    "            # find the average drawing time\n",
    "            avg_draw_time = all_drawings_for_this_func['Drawing time'].mean()\n",
    "            if(len(all_drawings_for_this_func) < 1):\n",
    "                # if no figures found, just continue onto the next\n",
    "                continue\n",
    "            # all of these entries have the same length (it's the same function)\n",
    "            # so just take the first one\n",
    "            _idx = all_drawings_for_this_func.index[0]\n",
    "            length = all_drawings_for_this_func['Length'].loc[_idx]\n",
    "            kappa = all_drawings_for_this_func['Kappa'].loc[_idx]\n",
    "            \n",
    "            if include_fig_embeddings is True:\n",
    "                # obtain embeddings for this figure from the CAE model\n",
    "                _, real_func = prepare_dataset_for_ae(all_drawings_for_this_func, len(X_FOR_ALL_FUNC))\n",
    "                real_func = torch.tensor(real_func)\n",
    "                ae_model.eval()\n",
    "                # kad budeš stigla: ovo commitaj, \n",
    "                # onda uz svaki crtež priloži njegovu duljinu i kappa\n",
    "                # i dodaš embedding crtanja\n",
    "                # imaj na umu da ti onda ne treba mean crtanja\n",
    "                # jer ce iskoristiti sve entryje (sve crteže)\n",
    "                # 25.8.2022, komentar na komentar iznad: nemam pojma što sam htjela reći\n",
    "                with torch.no_grad():\n",
    "                    embedding, _ = ae_model.get_latent(real_func)\n",
    "                    # take the first embedding since all embeddings will be the same\n",
    "                    # as all_drawings_for_this_func contains the same function\n",
    "                    embedding = np.array(embedding[0])\n",
    "                    embeddings_arr.append(embedding)\n",
    "\n",
    "            data_for_nn.loc[len(data_for_nn)] = [figureid, projectionid, length, kappa, avg_draw_time]\n",
    "\n",
    "    data_for_nn['Length'] = data_for_nn['Length'] / 30\n",
    "    data_for_nn['Kappa'] = data_for_nn['Kappa'] / 30\n",
    "    \n",
    "    if include_fig_embeddings is True:\n",
    "        # then this is an array of embeddings\n",
    "        # where each entry is an embedding for a figure\n",
    "        # now we have to join the dataframes with len/kappa and this\n",
    "        embeddings_arr = prep.minmax_scale(embeddings_arr)\n",
    "        # the way we join them is by pairwise multiplication:\n",
    "        # each embiedding is multiplied by length\n",
    "        # and each embedding is multiplied by kappa\n",
    "        # and the resulting two arrays are concatenated\n",
    "        # sidenote: embeddings_arr is an m x n matrix, where there are m curves\n",
    "        # and n is the length of a single embedding, WHILE data_for_nn has length m\n",
    "        # because there are m curves\n",
    "        embeddings_len = [emb * l for emb, l in zip(embeddings_arr, data_for_nn['Length'])]\n",
    "        embeddings_kap = [emb * k for emb, k in zip(embeddings_arr, data_for_nn['Kappa'])]\n",
    "        \n",
    "        data_for_nn = pd.concat(\n",
    "            [data_for_nn, pd.DataFrame(embeddings_len), pd.DataFrame(embeddings_kap)], \n",
    "            axis=1\n",
    "        )\n",
    "        # drop Length and Kappa because they are \"incorporated\" into embeddings now\n",
    "        data_for_nn.drop(['Kappa', 'Length'], axis=1, inplace=True)\n",
    "#         display(data_for_nn)\n",
    "    \n",
    "    X = data_for_nn.drop(['FigureID', 'Projection', 'AvgDrawTime'], axis=1)\n",
    "    Y = data_for_nn['AvgDrawTime']\n",
    "    display(X)\n",
    "    display(Y)\n",
    "    \n",
    "    X = np.array(X, dtype='float32')\n",
    "    Y = np.array(Y, dtype='float32')\n",
    "    return X, Y\n",
    "\n",
    "def do_everything_for_time_pred(df, include_fig_embeddings:bool=False):\n",
    "    trainx, trainy = prepare_dataset_for_time_pred(df.query('TestIndex == 0').copy(), include_fig_embeddings)\n",
    "    testx, testy = prepare_dataset_for_time_pred(df.query('TestIndex == 1').copy(), include_fig_embeddings)\n",
    "    \n",
    "    model = TimePredictionModel(hidden_layer_sizes=[128, 64, 32, 16, 8], in_size=np.shape(trainx)[1], dropout=0)\n",
    "    display(model)\n",
    "    \n",
    "    def loss_func(y, y_predicted, reduction='sum'):\n",
    "        rmspe = (torch.sqrt(torch.mean(torch.square((y_predicted - y) / y_predicted)))) * 100\n",
    "        if(reduction == 'sum'):\n",
    "            rmspe = torch.sum(rmspe)\n",
    "        else:\n",
    "            rmspe = torch.mean(rmspe)\n",
    "        return rmspe\n",
    "        \n",
    "    lr = 0.001\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    epochs = 200\n",
    "    batch_size = 1\n",
    "    \n",
    "    trainx_loader = torch.utils.data.DataLoader(torch.tensor(trainx), batch_size=batch_size)\n",
    "    trainy_loader = torch.utils.data.DataLoader(torch.tensor(trainy), batch_size=batch_size)\n",
    "    testx_loader = torch.utils.data.DataLoader(torch.tensor(testx), batch_size=batch_size)\n",
    "    testy_loader = torch.utils.data.DataLoader(torch.tensor(testy), batch_size=batch_size)\n",
    "    \n",
    "    def fit(model, xdataloader, ydataloader, optimizer, loss_func, validation:bool=False):\n",
    "        total_epoch_loss = 0.0\n",
    "        for x, y in zip(xdataloader, ydataloader):\n",
    "            if validation is False:\n",
    "                optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = loss_func(output, y)\n",
    "            \n",
    "            total_epoch_loss += loss\n",
    "\n",
    "            if validation is False:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        total_epoch_loss /= len(xdataloader.dataset)\n",
    "        return total_epoch_loss\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    for _epoch_idx in range(epochs):\n",
    "        # switch model to training mode\n",
    "        model.train()\n",
    "        train_epoch_loss = fit(model, trainx_loader, trainy_loader, optimizer, loss_func, False)\n",
    "        train_losses.append(train_epoch_loss)\n",
    "        \n",
    "        # and now val mode\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_epoch_loss = fit(model, testx_loader, testy_loader, optimizer, loss_func, True)\n",
    "            val_losses.append(val_epoch_loss)\n",
    "        \n",
    "        print(f'Epoch: {_epoch_idx+1:5} / {epochs};',\n",
    "              f' train loss: {train_epoch_loss:.5f}, val loss: {val_epoch_loss:.5f}')\n",
    "    \n",
    "    def plot_training_stats():\n",
    "        fig = plt.figure()\n",
    "        ax = fig.gca()\n",
    "        ax.plot(range(epochs), [loss.detach().numpy() for loss in train_losses], label='Train')\n",
    "        ax.plot(range(epochs), [loss.detach().numpy() for loss in val_losses], label='Val')\n",
    "        ax.set_title('Losses')\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    plot_training_stats()\n",
    "\n",
    "    return model, (trainx, trainy, testx, testy)\n",
    "\n",
    "time_pred_model, time_pred_datasets = do_everything_for_time_pred(\n",
    "    df.query('Device == \"Mouse\"'),\n",
    "    include_fig_embeddings=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e3bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predicted_vs_true(model, x, y_true):\n",
    "    fig = plt.figure(tight_layout=True, figsize=(5,5))\n",
    "    ax = fig.gca()\n",
    "    ax.axis('equal')\n",
    "    ax.grid(True)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(torch.tensor(x))\n",
    "        y_pred = y_pred.detach().numpy()\n",
    "        y_pred = np.ravel(y_pred)\n",
    "        print(y_true, y_pred, sep='\\n')\n",
    "        rmspe = (np.sqrt(np.mean(np.square((y_pred - y_true) / y_pred)))) * 100\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        mse = mean_squared_error(y_pred, y_true)\n",
    "        line = np.linspace(np.min(y_true),np.max(y_true))\n",
    "        ax.scatter(y_pred, y_true, color='lightblue', edgecolors='darkblue')\n",
    "        ax.plot(line, line, color='black')\n",
    "        ax.set_title(f'MSE: {mse}\\nRMSPE: {rmspe}')\n",
    "        ax.set_xlabel('Predicted [s]')\n",
    "        ax.set_ylabel('True [s]')\n",
    "    plt.show()\n",
    "\n",
    "plot_predicted_vs_true(time_pred_model, time_pred_datasets[2], time_pred_datasets[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8613f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
